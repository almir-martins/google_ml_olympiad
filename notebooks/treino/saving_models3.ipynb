{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas                   as pd\n",
    "import seaborn                  as sns\n",
    "import sweetviz                 as sv\n",
    "import matplotlib.pyplot        as plt\n",
    "import seaborn                  as sns\n",
    "import xgboost                  as xgb\n",
    "import lightgbm                 as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm                import SVC \n",
    "from sklearn                    import metrics\n",
    "from sklearn.pipeline           import Pipeline\n",
    "from sklearn.model_selection    import GridSearchCV\n",
    "# from sklearn.impute             import SimpleImputer\n",
    "# from sklearn.model_selection    import cross_validate\n",
    "from sklearn.model_selection    import train_test_split\n",
    "from sklearn.compose            import ColumnTransformer\n",
    "from sklearn.model_selection    import StratifiedKFold, KFold\n",
    "from sklearn.linear_model       import LogisticRegression\n",
    "from sklearn.preprocessing      import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing      import LabelEncoder, OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================================================================\n",
    "# Configura os gráficos\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    # %pylab inline\n",
    "    \n",
    "    plt.style.use('bmh')\n",
    "    plt.rcParams['figure.figsize'] = [22, 9]\n",
    "    plt.rcParams['font.size'] = 21\n",
    "\n",
    "    # display(HTML('<style>.conteiner{width:100% !important;}</style>'))\n",
    "\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    \n",
    "    # configura o pandas para quantidade de casas decimais\n",
    "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "    sns.set()\n",
    "\n",
    "jupyter_settings()\n",
    "\n",
    "# Calcula e retorna um dataframe com as métricas sem validação cruzada\n",
    "def simple_metrics(model_name, test, predict):\n",
    "    data = [\n",
    "        [\n",
    "            model_name,\n",
    "            metrics.precision_score(test, predict),\n",
    "            metrics.recall_score(test, predict),\n",
    "            metrics.f1_score(test, predict),\n",
    "            metrics.roc_auc_score(test, predict),\n",
    "        ]\n",
    "    ]\n",
    "    columns = [\"Model\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
    "    metrics_table = pd.DataFrame(data, columns=columns)\n",
    "    return metrics_table\n",
    "\n",
    "\n",
    "def tuning_hyperparams(pre_processor, modelo, model_name, param, X, y):\n",
    "    '''\n",
    "        Método que utiliza o Pipeline para pré processar os dados e calcula quais são \n",
    "        os melhores hiperparâmetros baseado no modelo e no dicionário de hiperparâmetros \n",
    "        informados. Retorna um dicionário com os parâmetros testados e os melhores valores\n",
    "        de cada um.\n",
    "\n",
    "        pre_processor: objeto da classe ColumnTransformer escolhido para os dados\n",
    "        modelo: instância do algoritimo a ser usado\n",
    "        model_name: String com o apelido do modelo a ser usado\n",
    "        param: Dict - dicionário com os hiperparams e os valores a serem testados\n",
    "        X: dados de treino\n",
    "        y: variável target dos dados de treino\n",
    "\n",
    "        return dict\n",
    "    '''\n",
    "    # criando o modelo usando pipeline\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", pre_processor),\n",
    "            (model_name, modelo),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Criando o dicionario de hiperparâmetros\n",
    "    test_keys = [model_name + '__' + x for x in param.keys()] # colocar duplo underscore entre o nome do modelo e o nome do parâmetro  (lr__)\n",
    "    test_values = list(param.values())\n",
    "    parameters = { test_keys[i]: test_values[i] for i in range(len(test_keys)) }\n",
    "\n",
    "    # Rodando 5-fold cross-validation com gridsearch\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # GridSearchCV\n",
    "    # grid = GridSearchCV(\n",
    "    #     model, param_grid=parameters, cv=kfold, n_jobs=-1, return_train_score=True\n",
    "    # )\n",
    "\n",
    "    # BayesSearch\n",
    "    bayes = BayesSearchCV(\n",
    "        model, search_spaces=parameters, cv=kfold, n_iter=20\n",
    "    )\n",
    "    \n",
    "    bayes.fit(X=X, y=y)\n",
    "    # grid.fit(X=X, y=y)\n",
    "\n",
    "    # Imprime os melhores parâmetros\n",
    "    param = bayes.best_params_\n",
    "    # param = grid.best_params_\n",
    "\n",
    "    return param\n",
    "\n",
    "\n",
    "def target_encoder(x, y, nome_coluna):\n",
    "    df_t = pd.DataFrame(x.values, columns=x.columns)\n",
    "    df_t['status'] = y\n",
    "    # display(df_t.head())\n",
    "\n",
    "    df_t = df_t[[nome_coluna, 'status']]\n",
    "\n",
    "    categories = df_t[nome_coluna].unique()\n",
    "    targets = df_t['status'].unique()\n",
    "\n",
    "    cat_list = list()\n",
    "    for cat in categories:\n",
    "        aux_dict = {}\n",
    "        aux_dict['category'] = cat\n",
    "        aux_df_t = df_t[df_t[nome_coluna] == cat]\n",
    "        counts = aux_df_t['status'].value_counts()\n",
    "        aux_dict['count'] = sum(counts)\n",
    "        \n",
    "        for t in targets:\n",
    "            aux_dict['status_' + str(t)] = counts[t]\n",
    "\n",
    "        cat_list.append(aux_dict)\n",
    "        \n",
    "    cat_list = pd.DataFrame(cat_list)\n",
    "    cat_list['resultado'] = cat_list['status_1'] / cat_list['count']\n",
    "\n",
    "    d = dict(zip( list(cat_list['category']), list(cat_list['resultado'])))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv('../data/test.csv')\n",
    "train_raw = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_do_caso</th>\n",
       "      <th>continente</th>\n",
       "      <th>educacao_do_empregado</th>\n",
       "      <th>tem_experiencia_de_trabalho</th>\n",
       "      <th>requer_treinamento_de_trabalho</th>\n",
       "      <th>num_de_empregados</th>\n",
       "      <th>ano_de_estabelecimento</th>\n",
       "      <th>regiao_de_emprego</th>\n",
       "      <th>salario_prevalecente</th>\n",
       "      <th>unidade_de_salario</th>\n",
       "      <th>posicao_em_tempo_integral</th>\n",
       "      <th>status_do_caso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EZYV10567</td>\n",
       "      <td>Europa</td>\n",
       "      <td>Ensino Médio</td>\n",
       "      <td>N</td>\n",
       "      <td>S</td>\n",
       "      <td>2087</td>\n",
       "      <td>1855</td>\n",
       "      <td>Sul</td>\n",
       "      <td>69711.24</td>\n",
       "      <td>Ano</td>\n",
       "      <td>S</td>\n",
       "      <td>Negado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EZYV5505</td>\n",
       "      <td>Ásia</td>\n",
       "      <td>Mestrado</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>5991</td>\n",
       "      <td>2003</td>\n",
       "      <td>Meio-Oeste</td>\n",
       "      <td>52931.38</td>\n",
       "      <td>Ano</td>\n",
       "      <td>S</td>\n",
       "      <td>Aprovado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EZYV5207</td>\n",
       "      <td>Ásia</td>\n",
       "      <td>Ensino Médio</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1426</td>\n",
       "      <td>2000</td>\n",
       "      <td>Ilha</td>\n",
       "      <td>110830.21</td>\n",
       "      <td>Ano</td>\n",
       "      <td>S</td>\n",
       "      <td>Negado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_do_caso continente educacao_do_empregado tem_experiencia_de_trabalho requer_treinamento_de_trabalho  num_de_empregados  ano_de_estabelecimento regiao_de_emprego  salario_prevalecente unidade_de_salario posicao_em_tempo_integral status_do_caso\n",
       "0  EZYV10567     Europa          Ensino Médio                           N                              S               2087                    1855               Sul              69711.24                Ano                         S         Negado\n",
       "1   EZYV5505       Ásia              Mestrado                           S                              N               5991                    2003        Meio-Oeste              52931.38                Ano                         S       Aprovado\n",
       "2   EZYV5207       Ásia          Ensino Médio                           N                              N               1426                    2000              Ilha             110830.21                Ano                         S         Negado"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_raw.copy()\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id', 'continente', 'escolaridade',\n",
    "       'tem_experiencia', 'requer_treinamento',\n",
    "       'num_empregados', 'ano_fundacao', 'regiao',\n",
    "       'salario_medio', 'periodicidade',\n",
    "       'tempo_integral', 'status']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas:   17836\n",
      "Quantidade de colunas:  12\n",
      "IDs únicos:             17836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conferindo a volumetria\n",
    "print(f\"Quantidade de linhas:   {df.shape[0]}\")\n",
    "print(f\"Quantidade de colunas:  {df.shape[1]}\")\n",
    "print(f\"IDs únicos:             {df.id.nunique()}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     object\n",
       "continente             object\n",
       "escolaridade           object\n",
       "tem_experiencia        object\n",
       "requer_treinamento     object\n",
       "num_empregados          int64\n",
       "ano_fundacao            int64\n",
       "regiao                 object\n",
       "salario_medio         float64\n",
       "periodicidade          object\n",
       "tempo_integral         object\n",
       "status                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Change Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Alterando os tipos de int64 e float64 para int32 e float32 respectivamente com o \n",
    "intuito de melhorar a performance além de alterar as variáveis nominais para o tipo \n",
    "category que consome menos memória que o tipo object.\n",
    "'''\n",
    "df.id = df.id.astype('string')\n",
    "df.continente = df.continente.astype('category')\n",
    "df.escolaridade = df.escolaridade.astype('category')\n",
    "df.tem_experiencia = df.tem_experiencia.astype('category')\n",
    "df.requer_treinamento = df.requer_treinamento.astype('category')\n",
    "df.num_empregados = df.num_empregados.astype('int32')\n",
    "df.ano_fundacao = df.ano_fundacao.astype('int32')\n",
    "df.regiao = df.regiao.astype('category')\n",
    "df.salario_medio = df.salario_medio.astype('float32')\n",
    "df.periodicidade = df.periodicidade.astype('category')\n",
    "df.tempo_integral = df.tempo_integral.astype('category')\n",
    "df.status = df.status.apply(lambda status: 0 if status == 'Negado' else 1)\n",
    "df.status = df.status.astype('int32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Check NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0.00\n",
       "continente           0.00\n",
       "escolaridade         0.00\n",
       "tem_experiencia      0.00\n",
       "requer_treinamento   0.00\n",
       "num_empregados       0.00\n",
       "ano_fundacao         0.00\n",
       "regiao               0.00\n",
       "salario_medio        0.00\n",
       "periodicidade        0.00\n",
       "tempo_integral       0.00\n",
       "status               0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Descriptive Statistical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os atributos entre numéricos e categóricos\n",
    "numerical_attributes = df.select_dtypes(include=['int32', 'float32'])\n",
    "categorical_attributes = df.select_dtypes(exclude=['int32', 'float32'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_empregados</th>\n",
       "      <td>17836.00</td>\n",
       "      <td>5726.50</td>\n",
       "      <td>23321.38</td>\n",
       "      <td>-26.00</td>\n",
       "      <td>1023.00</td>\n",
       "      <td>2117.00</td>\n",
       "      <td>3504.25</td>\n",
       "      <td>602069.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ano_fundacao</th>\n",
       "      <td>17836.00</td>\n",
       "      <td>1979.49</td>\n",
       "      <td>42.10</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>1976.00</td>\n",
       "      <td>1997.00</td>\n",
       "      <td>2005.00</td>\n",
       "      <td>2016.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salario_medio</th>\n",
       "      <td>17836.00</td>\n",
       "      <td>74327.42</td>\n",
       "      <td>52795.86</td>\n",
       "      <td>2.14</td>\n",
       "      <td>33892.91</td>\n",
       "      <td>70106.92</td>\n",
       "      <td>107564.71</td>\n",
       "      <td>319210.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <td>17836.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count     mean      std     min      25%      50%       75%       max\n",
       "num_empregados 17836.00  5726.50 23321.38  -26.00  1023.00  2117.00   3504.25 602069.00\n",
       "ano_fundacao   17836.00  1979.49    42.10 1800.00  1976.00  1997.00   2005.00   2016.00\n",
       "salario_medio  17836.00 74327.42 52795.86    2.14 33892.91 70106.92 107564.71 319210.28\n",
       "status         17836.00     0.67     0.47    0.00     0.00     1.00      1.00      1.00"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_attributes.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valores únicos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tem_experiencia</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>requer_treinamento</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo_integral</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>escolaridade</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>periodicidade</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regiao</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continente</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>17836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Valores únicos\n",
       "tem_experiencia                  2\n",
       "requer_treinamento               2\n",
       "tempo_integral                   2\n",
       "escolaridade                     4\n",
       "periodicidade                    4\n",
       "regiao                           5\n",
       "continente                       6\n",
       "id                           17836"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display( pd.DataFrame( categorical_attributes.apply(lambda x: x.unique().shape[0]), columns=[ 'Valores únicos'], ).sort_values('Valores únicos'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Target Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11937\n",
       "0     5899\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.status.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "# dividindo em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df2.drop(columns=[\"id\", \"status\"], axis=1), df2[\"status\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ======================== PRÉ PROCESSAMENTO ===============================\n",
    "def pre_processa(data):\n",
    "    df_pre = data.copy()\n",
    "    # pipeline para pré-processamento das variáveis binárias\n",
    "    for col in df_pre[['tem_experiencia', 'requer_treinamento', 'tempo_integral']].columns:\n",
    "        df_pre[col] = df_pre[col].apply(lambda x: 0 if x=='N' else 1)\n",
    "\n",
    "    # Criar feature de salário mensal\n",
    "    df_pre['salario_medio_mes'] = df_pre.apply( lambda linha: \n",
    "                                        linha['salario_medio'] / 12 if (linha['periodicidade']=='Ano') else \n",
    "                                        linha['salario_medio'] if (linha['periodicidade']=='Mês') else \n",
    "                                        linha['salario_medio'] * 4         if (linha['periodicidade']=='Semana') else \n",
    "                                        linha['salario_medio'] * 8 * 30    if ((linha['periodicidade']=='Hora') & (linha['tempo_integral']==1)) else \n",
    "                                        linha['salario_medio'] * 4 * 30    if ((linha['periodicidade']=='Hora') & (linha['tempo_integral']==0)) \n",
    "                                        else 0, \n",
    "                                        axis=1)\n",
    "    df_pre.drop(columns=['salario_medio', 'periodicidade'], inplace=True)\n",
    "\n",
    "    # Corrigir empresas com qtd de funcionários negativos\n",
    "    df_pre.num_empregados = df_pre.num_empregados.apply( lambda x: df_pre.num_empregados.median() if x < 1 else x)\n",
    "\n",
    "    # Criar categorias para quantidade de funcionários (micro, pequena, média, grande):\n",
    "    # 1-até 100; 2-até 1000; 3-até 5000; 4-maior que 5000;\n",
    "    df_pre.num_empregados = df_pre.num_empregados.apply(lambda x: 0 if x<= 100 else 1 if x <= 1000 else 2 if x <= 5000 else 3)\n",
    "\n",
    "    # Criar categorias para o ano de fundação (antiga, consolidada, nova):\n",
    "    # 3-anterior a 1950; 2-anterior a 2000; 1-posterior a 2000;\n",
    "    df_pre.ano_fundacao = df_pre.ano_fundacao.apply(lambda x: 2 if x <= 1950 else 1 if x <= 2000 else 0)\n",
    "\n",
    "    # pipeline para pré-processamento das variáveis categóricas\n",
    "    # Usando ordinal encoder\n",
    "    # ord_enc = OrdinalEncoder()\n",
    "    # df_pre[['continente', 'escolaridade', 'regiao', ]] = ord_enc.fit_transform(df_pre[['continente', 'escolaridade', 'regiao', ]])\n",
    "\n",
    "    # Usando TargetEncoder\n",
    "    df_x = pd.DataFrame(df2.values, columns=df2.columns)\n",
    "    df_y = df2.status\n",
    "    # feature continente\n",
    "    dicionario = target_encoder(df_x, df_y, 'continente')\n",
    "    df_pre['continente'] = df_pre['continente'].map(dicionario)\n",
    "    # feature escolaridade\n",
    "    dicionario = target_encoder(df_x, df_y, 'escolaridade')\n",
    "    df_pre['escolaridade'] = df_pre['escolaridade'].map(dicionario)\n",
    "    # feature regiao\n",
    "    dicionario = target_encoder(df_x, df_y, 'regiao')\n",
    "    df_pre['regiao'] = df_pre['regiao'].map(dicionario)\n",
    "\n",
    "\n",
    "    # pipeline para pré-processamento das variáveis binárias\n",
    "    # ohe_enc = OneHotEncoder()\n",
    "    # df_pre[['tem_experiencia', 'requer_treinamento', 'tempo_integral']] = ohe_enc.fit_transform(df_pre[['tem_experiencia', 'requer_treinamento', 'tempo_integral']])\n",
    "\n",
    "    # Salário categorizado\n",
    "    df_pre.salario_medio_mes = df_pre.salario_medio_mes.apply(lambda x: \n",
    "                                                              7 if x <= 2000 else \n",
    "                                                              6 if x <= 4000 else \n",
    "                                                              5 if x <= 6000 else \n",
    "                                                              4 if x <= 8000 else \n",
    "                                                              3 if x <= 10000 else \n",
    "                                                              2 if x <= 20000 else 1)\n",
    "    \n",
    "\n",
    "    return df_pre\n",
    "\n",
    "# Pré processa os dados\n",
    "X_train = pre_processa(X_train)\n",
    "\n",
    "X_test = pre_processa(X_test)\n",
    "df3 = pre_processa( df2 )\n",
    "\n",
    "# pipeline para pré-processamento das variáveis binárias\n",
    "encode_transformer = Pipeline(steps=[('encoder', OneHotEncoder(drop='if_binary'))])\n",
    "\n",
    "# pipeline para pré-processamento das variáveis categóricas\n",
    "scale_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "# Compondo os pré-processadores\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ohe\", encode_transformer, list( df3.drop(columns=[\"id\", \"status\", \"continente\", \"escolaridade\", \"num_empregados\", \"ano_fundacao\", \"regiao\", \"salario_medio_mes\", \"status\" ]))),\n",
    "        (\"scaler\", scale_transformer, list( df3.drop(columns=[\"id\", \"status\"]))),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'lr'\n",
    "# modelo = LogisticRegression()\n",
    "# parametros = { \n",
    "#     \"C\":            [0.1, 1.0, 10.0],\n",
    "#     \"penalty\":      [\"l2\",  ],\n",
    "#     \"tol\":          [0.0001, 0.001, 0.01],\n",
    "#     'solver':       ['liblinear', \"saga\", \"newton-cg\", \"sag\", \"lbfgs\"], # \n",
    "#     \"max_iter\":     [1, 5, 10],\n",
    "#     \"class_weight\": [\"balanced\", None],\n",
    "# }\n",
    "\n",
    "# # O cross validation para descobrir os melhores parametros é realizado no dataset completo\n",
    "# best_params_lr = tuning_hyperparams(preprocessor, modelo, model_name, parametros, df3.drop(columns=['id', 'status']), df3[\"status\"])\n",
    "\n",
    "# # Usando os melhores parâmetros do Cross validation\n",
    "# model_lr = Pipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\n",
    "#             \"lr\",\n",
    "#             LogisticRegression(\n",
    "#                 C               = best_params_lr[model_name + '__' + \"C\"],\n",
    "#                 penalty         = best_params_lr[model_name + '__' + \"penalty\"],\n",
    "#                 tol             = best_params_lr[model_name + '__' + \"tol\"],\n",
    "#                 solver          = best_params_lr[model_name + '__' + \"solver\"],\n",
    "#                 max_iter        = best_params_lr[model_name + '__' + \"max_iter\"],\n",
    "#                 class_weight    = best_params_lr[model_name + '__' + \"class_weight\"],\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # treinando o modelo\n",
    "# model_lr.fit(X_train, y_train)\n",
    "\n",
    "# # testando o modelo\n",
    "# y_pred = model_lr.predict(X_test)\n",
    "\n",
    "# # Salvando os melhores parametros\n",
    "# with open('../models/lr.pickle', 'wb') as handle:\n",
    "#     pickle.dump(best_params_lr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Imprimindo as métricas\n",
    "# lr_metrics_table = simple_metrics('Logistic Regression', y_test, y_pred)\n",
    "# lr_metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_params_lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Suport Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'svm'\n",
    "# modelo = SVC(kernel='linear', C=2, gamma=1)\n",
    "\n",
    "# parametros = { \n",
    "#     \"kernel\":       ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "#     \"C\":            [0.1, 1.0, 10.0],\n",
    "#     \"gamma\":        [0.1, 1.0, 10.0],       #'scale', 'auto', \n",
    "#     'degree':       [3, 4, 5],\n",
    "#     'coef0':        [0.0, 0.2, 0.4],\n",
    "#     \"class_weight\": [\"balanced\", None],\n",
    "# }\n",
    "\n",
    "# # O cross validation para descobrir os melhores parametros é realizado no dataset completo\n",
    "# best_params_svm = tuning_hyperparams(preprocessor, modelo, model_name, parametros, df3.drop([\"id\", \"status\"], axis=1), df3[\"status\"])\n",
    "\n",
    "# # Usando os melhores parâmetros do Cross validation\n",
    "# model_svm = Pipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\n",
    "#             \"svm\",\n",
    "#             SVC(\n",
    "#                 kernel          = best_params_svm[model_name + '__' + \"kernel\"],\n",
    "#                 C               = best_params_svm[model_name + '__' + \"C\"],\n",
    "#                 gamma           = best_params_svm[model_name + '__' + \"gamma\"],\n",
    "#                 degree          = best_params_svm[model_name + '__' + \"degree\"],\n",
    "#                 coef0           = best_params_svm[model_name + '__' + \"coef0\"],\n",
    "#                 class_weight    = best_params_svm[model_name + '__' + \"class_weight\"],\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # treinando o modelo\n",
    "# model_svm.fit(X_train, y_train)\n",
    "\n",
    "# # testando o modelo\n",
    "# y_pred = model_svm.predict(X_test)\n",
    "\n",
    "# # Salvando os melhores parametros\n",
    "# with open('../models/svm.pickle', 'wb') as handle:\n",
    "#     pickle.dump(best_params_svm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Imprimindo as metricas\n",
    "# svm_metrics_table = simple_metrics('SVM', y_test, y_pred)\n",
    "# svm_metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_params_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Precision  Recall   F1  AUC\n",
       "0  xgboost       0.79    0.86 0.83 0.69"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'xgboost'\n",
    "\n",
    "modelo = xgb.XGBClassifier(\n",
    "    n_estimators=10000,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eta=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "parametros = { \n",
    "    'learning_rate':    [0.1, 0.01, 0.001],\n",
    "    \"n_estimators\":     [10, 500, 1000],\n",
    "    \"max_depth\":        [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'reg_alpha':        [0.1, 0.01, 0.001],\n",
    "    'reg_lambda':       [0.1, 0.01, 0.001],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    \"objective\":        [\"binary:logistic\",\"binary:logitraw\", \"binary:hinge\"],\n",
    "    'tree_method':      ['hist', 'approx'],\n",
    "    \"eta\":              [0.1, 0.5, 0.7],\n",
    "    \"random_state\":     [42],\n",
    "    \"subsample\":        [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# O cross validation para descobrir os melhores parametros é realizado no dataset completo\n",
    "best_params_xgb = tuning_hyperparams(preprocessor, modelo, model_name, parametros, df3.drop([\"id\", \"status\"], axis=1), df3[\"status\"])\n",
    "\n",
    "# Usando os melhores parâmetros do Cross validation\n",
    "model_xgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"xbg\",\n",
    "            xgb.XGBClassifier(\n",
    "                learning_rate       = best_params_xgb[model_name + '__' + \"learning_rate\"],\n",
    "                n_estimators        = best_params_xgb[model_name + '__' + \"n_estimators\"],\n",
    "                max_depth           = best_params_xgb[model_name + '__' + \"max_depth\"],\n",
    "                min_child_weight    = best_params_xgb[model_name + '__' + \"min_child_weight\"],\n",
    "                reg_alpha           = best_params_xgb[model_name + '__' + \"reg_alpha\"],\n",
    "                reg_lambda          = best_params_xgb[model_name + '__' + \"reg_lambda\"],\n",
    "                colsample_bytree    = best_params_xgb[model_name + '__' + \"colsample_bytree\"],\n",
    "                objective           = best_params_xgb[model_name + '__' + \"objective\"],\n",
    "                tree_method         = best_params_xgb[model_name + '__' + \"tree_method\"],\n",
    "                eta                 = best_params_xgb[model_name + '__' + \"eta\"],\n",
    "                random_state        = best_params_xgb[model_name + '__' + \"random_state\"],\n",
    "                subsample           = best_params_xgb[model_name + '__' + \"subsample\"],\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# treinando o modelo\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# testando o modelo\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "\n",
    "# Salvando os melhores parametros\n",
    "with open('../models/xgb.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_xgb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Imprimindo as métricas\n",
    "xgb_metrics_table = simple_metrics('xgboost', y_test, y_pred)\n",
    "xgb_metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('xgboost__colsample_bytree', 0.7), ('xgboost__eta', 0.7), ('xgboost__learning_rate', 0.01), ('xgboost__max_depth', 5), ('xgboost__min_child_weight', 5), ('xgboost__n_estimators', 1000), ('xgboost__objective', 'binary:logistic'), ('xgboost__random_state', 42), ('xgboost__reg_alpha', 0.01), ('xgboost__reg_lambda', 0.01), ('xgboost__subsample', 0.7), ('xgboost__tree_method', 'hist')])\n"
     ]
    }
   ],
   "source": [
    "print(best_params_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Precision  Recall   F1  AUC\n",
       "0  lgbm       0.83    0.74 0.79 0.71"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'lgbm'\n",
    "\n",
    "modelo = lgb.LGBMClassifier()\n",
    "\n",
    "parametros = { \n",
    "    \"class_weight\":         ['balanced'],\n",
    "    'objective':            ['binary'],\n",
    "    \"boosting_type\":        ['gbdt', 'dart'],\n",
    "    \"n_estimators\":         [100, 500, 1000],\n",
    "    \"max_depth\":            [3, 5, 7, 10],\n",
    "    \"random_state\":         [42], \n",
    "    'learning_rate':        [0.1, 0.01, 0.001],\n",
    "    'min_child_samples':    [1, 5, 10],\n",
    "    'subsample':            [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree':     [0.5, 0.7, 0.9],\n",
    "    'reg_alpha':            [0.1, 0.01, 0.001],\n",
    "    'reg_lambda':           [0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "# O cross validation para descobrir os melhores parametros é realizado no dataset completo\n",
    "best_params_lgbm = tuning_hyperparams(preprocessor, modelo, model_name, parametros, df3.drop([\"id\", \"status\"], axis=1), df3[\"status\"])\n",
    "\n",
    "# Usando os melhores parâmetros do Cross validation\n",
    "model_lgbm = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"lgbm\",\n",
    "            lgb.LGBMClassifier(\n",
    "                class_weight        = best_params_lgbm[model_name + '__' + \"class_weight\"],\n",
    "                objective           = best_params_lgbm[model_name + '__' + \"objective\"],\n",
    "                boosting_type       = best_params_lgbm[model_name + '__' + \"boosting_type\"],\n",
    "                n_estimators        = best_params_lgbm[model_name + '__' + \"n_estimators\"],\n",
    "                max_depth           = best_params_lgbm[model_name + '__' + \"max_depth\"],\n",
    "                random_state        = best_params_lgbm[model_name + '__' + \"random_state\"],\n",
    "                learning_rate       = best_params_lgbm[model_name + '__' + \"learning_rate\"],\n",
    "                min_child_samples   = best_params_lgbm[model_name + '__' + \"min_child_samples\"],\n",
    "                subsample           = best_params_lgbm[model_name + '__' + \"subsample\"],\n",
    "                colsample_bytree    = best_params_lgbm[model_name + '__' + \"colsample_bytree\"],\n",
    "                reg_alpha           = best_params_lgbm[model_name + '__' + \"reg_alpha\"],\n",
    "                reg_lambda          = best_params_lgbm[model_name + '__' + \"reg_lambda\"],\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# treinando o modelo\n",
    "model_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# testando o modelo\n",
    "y_pred = model_lgbm.predict(X_test)\n",
    "\n",
    "# Salvando os melhores parametros\n",
    "with open('../models/lgbm.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_lgbm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Imprimindo as métricas\n",
    "lgbm_metrics_table = simple_metrics('lgbm', y_test, y_pred)\n",
    "lgbm_metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('lgbm__boosting_type', 'gbdt'), ('lgbm__class_weight', 'balanced'), ('lgbm__colsample_bytree', 0.7), ('lgbm__learning_rate', 0.01), ('lgbm__max_depth', 7), ('lgbm__min_child_samples', 1), ('lgbm__n_estimators', 100), ('lgbm__objective', 'binary'), ('lgbm__random_state', 42), ('lgbm__reg_alpha', 0.01), ('lgbm__reg_lambda', 0.001), ('lgbm__subsample', 0.7)])\n"
     ]
    }
   ],
   "source": [
    "print(best_params_lgbm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gradient</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Precision  Recall   F1  AUC\n",
       "0  gradient       0.73    0.89 0.80 0.59"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gradient'\n",
    "\n",
    "modelo = GradientBoostingClassifier()\n",
    "\n",
    "parametros = { \n",
    "    'learning_rate':    [0.1, 0.01, 0.001],\n",
    "    \"n_estimators\":     [10, 500, 1000],\n",
    "    \"max_depth\":        [3, 5, 7, 10],\n",
    "    \"subsample\":        [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"max_features\":     [0.5, 0.7, 0.9],\n",
    "    'min_samples_split':[2, 5, 7, 10],\n",
    "    \"min_samples_leaf\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    \"min_weight_fraction_leaf\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"criterion\":        [\"friedman_mse\", \"squared_error\"],\n",
    "    \"random_state\":     [42],\n",
    "}\n",
    "\n",
    "# O cross validation para descobrir os melhores parametros é realizado no dataset completo\n",
    "best_params_gradient = tuning_hyperparams(preprocessor, modelo, model_name, parametros, df3.drop([\"id\", \"status\"], axis=1), df3[\"status\"])\n",
    "\n",
    "# Usando os melhores parâmetros do Cross validation\n",
    "model_gradient = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"gradient\",\n",
    "            GradientBoostingClassifier(\n",
    "                learning_rate       = best_params_gradient[model_name + '__' + \"learning_rate\"],\n",
    "                n_estimators        = best_params_gradient[model_name + '__' + \"n_estimators\"],\n",
    "                max_depth           = best_params_gradient[model_name + '__' + \"max_depth\"],                \n",
    "                subsample           = best_params_gradient[model_name + '__' + \"subsample\"],\n",
    "                max_features        = best_params_gradient[model_name + '__' + \"max_features\"],\n",
    "                min_samples_split   = best_params_gradient[model_name + '__' + \"min_samples_split\"],\n",
    "                min_samples_leaf    = best_params_gradient[model_name + '__' + \"min_samples_leaf\"],                \n",
    "                min_weight_fraction_leaf = best_params_gradient[model_name + '__' + \"min_weight_fraction_leaf\"],\n",
    "                criterion           = best_params_gradient[model_name + '__' + \"criterion\"],\n",
    "                random_state        = best_params_gradient[model_name + '__' + \"random_state\"],\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# treinando o modelo\n",
    "model_gradient.fit(X_train, y_train)\n",
    "\n",
    "# testando o modelo\n",
    "y_pred = model_gradient.predict(X_test)\n",
    "\n",
    "# Salvando os melhores parametros\n",
    "with open('../models/gradient.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_gradient, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Imprimindo as métricas\n",
    "gradient_metrics_table = simple_metrics('gradient', y_test, y_pred)\n",
    "gradient_metrics_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('../models/lr.pickle', 'wb') as handle:\n",
    "#     pickle.dump(best_params_lr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('../models/svm.pickle', 'wb') as handle:\n",
    "#     pickle.dump(best_params_svm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../models/xgb.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_xgb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../models/lgbm.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_lgbm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../models/gradien.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_gradient, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gradient</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Precision  Recall   F1  AUC\n",
       "0   xgboost       0.79    0.86 0.83 0.69\n",
       "1      lgbm       0.83    0.74 0.79 0.71\n",
       "2  gradient       0.73    0.89 0.80 0.59"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics_table = pd.concat([lr_metrics_table, svm_metrics_table, xgb_metrics_table, lgbm_metrics_table, gradient_metrics_table], ignore_index=True)\n",
    "metrics_table = pd.concat([xgb_metrics_table, lgbm_metrics_table, gradient_metrics_table], ignore_index=True)\n",
    "metrics_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f497c605414ea2cafdc7801f144bc5be6c06a960c44efbf4c101b1bd9e2ace56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
